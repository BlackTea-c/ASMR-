

# 最优情况 -> 有台本 直接翻译！
# 没台本 -> 音频识别+翻译 

[spacy教程,很友善](https://course.spacy.io/zh/)
# 制作ing

[懂得都懂](1.png)

本质上，现在的大模型要解决的问题，就是一个序列数据转换的问题：
输入序列 X = [x1, x2, ..., xm]， 输出序列Y = [y1, y2, …, yn]，X和Y之间的关系是：Y = WX。
“模型”指的就是上述公式中的矩阵W。
在这里，矩阵W就是通过机器学习，得出的用来将X序列，转换成Y序列的权重参数组成的矩阵。
需要特别说明：这里为了方便理解，做了大量的简化。在实际的模型中，会有多个用于不同目的的权重参数矩阵，也还有一些其它参数。

从参数规模的角度，大模型的微调分成两条技术路线：

一条是对全量的参数，进行全量的训练，这条路径叫全量微调FFT(Full Fine Tuning)。

一条是只对部分的参数进行训练，这条路径叫PEFT(Parameter-Efficient Fine Tuning)。

FFT的原理，就是用特定的数据，对大模型进行训练，将W变成W`，W`相比W ，最大的优点就是上述特定数据领域的表现会好很多。

但FFT也会带来一些问题，影响比较大的问题，主要有以下两个：

一个是训练的成本会比较高，因为微调的参数量跟预训练的是一样的多的；

一个是叫灾难性遗忘(Catastrophic Forgetting)，用特定训练数据去微调可能会把这个领域的表现变好，但也可能会把原来表现好的别的领域的能力变差。

PEFT主要想解决的问题，就是FFT存在的上述两个问题，PEFT也是目前比较主流的微调方案。

从训练数据的来源、以及训练的方法的角度，大模型的微调有以下几条技术路线：

一个是监督式微调SFT(Supervised Fine Tuning) ，这个方案主要是用人工标注的数据，用传统机器学习中监督学习的方法，对大模型进行微调；

一个是基于人类反馈的强化学习微调RLHF(Reinforcement Learning with Human Feedback) ，这个方案的主要特点是把人类的反馈，通过强化学习的方式，引入到对大模型的微调中去，让大模型生成的结果，更加符合人类的一些期望；

还有一个是基于AI反馈的强化学习微调RLAIF(Reinforcement Learning with AI Feedback) ，这个原理大致跟RLHF类似，但是反馈的来源是AI。这里是想解决反馈系统的效率问题，因为收集人类反馈，相对来说成本会比较高、效率比较低。

不同的分类角度，只是侧重点不一样，对同一个大模型的微调，也不局限于某一个方案，可以多个方案一起。

微调的最终目的，是能够在可控成本的前提下，尽可能地提升大模型在特定领域的能力。
